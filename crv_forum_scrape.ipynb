{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from requests import get\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the thread links for the current page\n",
    "def page_posts(page_url):\n",
    "    response = get(page_url)\n",
    "    #throw warning for status codes that are not 200\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    posts = html_soup.find_all('div', class_=\"california-thread-item\")\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape individual thread given a thread url\n",
    "def thread_posts(thread_url):\n",
    "    response = get(thread_url)\n",
    "    #throw warning for status codes that are not 200\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    posts = html_soup.find_all('div', class_=\"bbWrapper\")\n",
    "    reply_count = html_soup.find('span', class_='count').text\n",
    "    if 'K' in reply_count:\n",
    "        reply_count = reply_count[:-1]+'000'\n",
    "    pages = int(reply_count)//20 +1\n",
    "    if pages>1:\n",
    "        for i in range(pages-1):\n",
    "            link2 = thread_url + 'page-%d' %(i+2)\n",
    "            response = get(link2)\n",
    "            html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            posts += html_soup.find_all('div', class_=\"bbWrapper\")\n",
    "    return posts\n",
    "\n",
    "# filter out unwanted text\n",
    "def filer_thread_post(posts):\n",
    "    docs = []\n",
    "    for i in range(len(posts)):\n",
    "        text = posts[i].text\n",
    "        tok = 'Click to expand...'\n",
    "        if tok in text:\n",
    "            index =  text.find(tok)\n",
    "            text = text[index+len(tok)+1:]\n",
    "        text = text.replace('\\n',' ').replace('...',' ').replace('..',' ')\n",
    "        if len(text)*' ' != text:\n",
    "            docs.append(text.replace('\\n',' ').replace('...',' ').replace('..',' '))\n",
    "    return docs\n",
    "\n",
    "#get posts from a thread\n",
    "def get_thread_docs(thread_url):\n",
    "    posts = thread_posts(thread_url)\n",
    "    docs = filer_thread_post(posts)\n",
    "    #docs.append(thread_url)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_post_info(post):\n",
    "    reply_count = post.find('div', class_= 'reply-count').text.strip()\n",
    "    view_count = post.find('div', class_='structItem-cell forum-view-stats view-count-cell').text.strip()\n",
    "    user = post.find('a', class_=\"username\")\n",
    "    user_id = user['data-user-id']\n",
    "    user_name = user.text\n",
    "    user_join_date = post.find('li', class_=\"structItem-startDate\").find('a').find('time')['datetime']\n",
    "\n",
    "    title_head = post.find('div', class_= \"structItem-title\") \n",
    "    post_title = title_head.text.strip()\n",
    "    post_url = 'https://www.crvownersclub.com' + title_head.find('a')['href']\n",
    "    return user_id, user_name, user_join_date, post_title, reply_count, view_count, post_url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258.9506561756134\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "post_title = []\n",
    "post_url = []\n",
    "reply_count = []\n",
    "view_count = []\n",
    "join_date = []\n",
    "user_name = []\n",
    "user_id = []\n",
    "for i in range(360): #360 is the total number of pages in the Problem and issue forum\n",
    "    page_i_url ='https://www.crvownersclub.com/forums/problems-issues.14/page-%d' %i\n",
    "    page_i_posts = page_posts(page_i_url)\n",
    "    for post in page_i_posts:\n",
    "        post_info = main_post_info(post)\n",
    "        uid, uname, ujoin_date, title, reply, view, url = post_info\n",
    "        if title[:7]=='Sticky\\n':\n",
    "            title = title[8:]\n",
    "        post_title.append(title)\n",
    "        post_url.append(url)\n",
    "        reply_count.append(reply)\n",
    "        view_count.append(view)\n",
    "        join_date.append(ujoin_date)\n",
    "        user_name.append(uname)\n",
    "        user_id.append(uid)\n",
    "            \n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buidling data frame to store thread statistics\n",
    "df_posts_p360 = pd.DataFrame({'user name': user_name,\n",
    "                         'user id': user_id,\n",
    "                         'user join date': join_date,\n",
    "                         'post title': post_title,\n",
    "                         'post url': post_url,\n",
    "                         'reply count': reply_count,\n",
    "                         'view count': view_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump( df_posts_p360, open(  \"df_posts_p360.p\", \"wb\" ) )\n",
    "df_posts_p360 = pickle.load( open( \"df_posts_p360.p\", \"rb\" ) )\n",
    "df_posts_p10 = pickle.load( open( \"df_posts_p10.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = []\n",
    "df_posts = df_posts_p360\n",
    "size = df_posts['post url'].size\n",
    "all_docs = []\n",
    "for i in range(size):\n",
    "    try:\n",
    "        post_url = df_posts['post url'].iloc[i]\n",
    "        docs = get_thread_docs(post_url)\n",
    "        docs.append(post_url)\n",
    "        all_docs.append(i)\n",
    "        #if i % 5000 == 0:\n",
    "        #    pickle.dump( all_docs, open(\"all_docs_%i.p\" %i, \"wb\" ) )\n",
    "    except:\n",
    "        print('there is something wroing with getting doc ', i)\n",
    "        continue\n",
    "    print(i+1, '/', size, len(docs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = df_posts_p360\n",
    "size = df_posts['post url'].size\n",
    "all_docsX = []\n",
    "for i in range(size, size):\n",
    "    try:\n",
    "        post_url = df_posts['post url'].iloc[i]\n",
    "        post_title = df_posts['post title'].iloc[i]\n",
    "        docs = get_thread_docs(post_url)\n",
    "        docs.append(post_title)\n",
    "        docs.append(post_url)\n",
    "        all_docsC.append(docs)\n",
    "    except:\n",
    "        print('there is something wroing with getting doc i: ', i)\n",
    "        continue\n",
    "    print(i+1, '/', size, len(docs)) \n",
    "    \n",
    "#pickle.dump( all_docsC, open(\"all_docsC.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_docsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_docsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12610"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = all_docsA + all_docsB + all_docsC\n",
    "pickle.dump( all_docs, open(\"all_docs.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
