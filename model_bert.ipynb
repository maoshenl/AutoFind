{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 5732083.41B/s]\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_posts = pickle.load( open( \"df_posts_p360.p\", \"rb\" ) )\n",
    "all_docs = pickle.load( open(\"all_docs.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findall(p, s):\n",
    "    '''Yields all the positions of\n",
    "    the pattern p in the string s.'''\n",
    "    i = s.find(p)\n",
    "    while i != -1:\n",
    "        yield i\n",
    "        i = s.find(p, i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(paragraph):\n",
    "    # convert input corpus to lower case.   \n",
    "    corpus = paragraph.lower()\n",
    "    # collecting a list of stop words from nltk and punctuation form\n",
    "    corpus = corpus.replace('.','. [SEP]').replace('?','? [SEP]').replace('!','! [SEP]')\n",
    "    corpus = corpus.replace('(','[SEP] (').replace(')',') [SEP]')\n",
    "    corpus = unidecode(corpus)\n",
    "\n",
    "    tok = 'Click to expand'\n",
    "    if tok in corpus:\n",
    "        index = corpus.find(tok)\n",
    "        corpus = corpus[index+len(tok)+1:]\n",
    "    \n",
    "    for i in range(2,10):\n",
    "        corpus = corpus.replace(i*' ', ' ')\n",
    "        \n",
    "    if corpus[-5:] != '[SEP]':\n",
    "        corpus += ' [SEP]'\n",
    "\n",
    "    #print('----')\n",
    "    #print(corpus)\n",
    "    #corpus = '[CLS] '+ corpus\n",
    "    \n",
    "    #replace '[SEP]' with '[CLS]' every other time.\n",
    "    result = []\n",
    "    split_corp  = corpus.split('[SEP]')\n",
    "    split_corp2 = [ split_corp[i:i+2] for i in range(0, len(split_corp), 2) ]\n",
    "    for i, sc in enumerate(split_corp2):\n",
    "        sent = '[SEP]'.join(sc)\n",
    "        if sent[:5] !=['[CLS]']:\n",
    "            sent = '[CLS] ' +sent if i==0 else '[CLS]' +sent\n",
    "        if sent[-5:] != '[SEP]':\n",
    "            sent += ' [SEP]'\n",
    "        sent = sent.replace('  ',' ')\n",
    "        if sent != '[CLS] [SEP]':\n",
    "            result.append( sent )\n",
    "    corpus = result[:]\n",
    "    \n",
    "    #print(corpus)\n",
    "    #print('XXXXX')\n",
    "    tokenized_text = [tokenizer.tokenize(c) for c in corpus]\n",
    "    seg_ids = [(tt.index('[SEP]')+1)*[0] + (len(tt)-tt.index('[SEP]')-1)*[1] for tt in tokenized_text]\n",
    "    indexed_tokens = [tokenizer.convert_tokens_to_ids(tt) for tt in tokenized_text]\n",
    "    \n",
    "    lens = [len(tt) for tt in tokenized_text]\n",
    "    if max(lens)>500:\n",
    "        print(corpus)\n",
    "    \n",
    "    '''if True:\n",
    "        print(corpus)\n",
    "        #print('before')\n",
    "        tokenized_text = tokenizer.tokenize(corpus)\n",
    "        #print('mid', len(corpus))\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        #print('after')\n",
    "        print('-----', len(corpus))\n",
    "        print(len(tokenized_text), len(indexed_tokens))\n",
    "        #tokenized_text, indexed_tokens = [], []\n",
    "    \n",
    "    tokenized_text = tokenizer.tokenize(corpus)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)'''\n",
    "    \n",
    "    return tokenized_text, indexed_tokens, seg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process0(paragraph):\n",
    "    # convert input corpus to lower case.   \n",
    "    corpus = paragraph.lower()\n",
    "    # collecting a list of stop words from nltk and punctuation form\n",
    "    corpus = corpus.replace('.','. [SEP]').replace('?','? [SEP]').replace('!','! [SEP]')\n",
    "    corpus = corpus.replace('(','[SEP] (').replace(')',') [SEP]')\n",
    "    corpus = unidecode(corpus)\n",
    "\n",
    "    tok = 'Click to expand'\n",
    "    if tok in corpus:\n",
    "        index = corpus.find(tok)\n",
    "        corpus = corpus[index+len(tok)+1:]\n",
    "    \n",
    "    for i in range(2,10):\n",
    "        corpus = corpus.replace(i*' ', ' ')\n",
    "        \n",
    "    if corpus[-5:] != '[SEP]':\n",
    "        corpus += ' [SEP]'\n",
    "\n",
    "    #replace '[SEP]' with '[CLS]' every other time.\n",
    "    result = []\n",
    "    split_corp  = corpus.split('[SEP]')\n",
    "    split_corp2 = [ split_corp[i:i+2] for i in range(0, len(split_corp), 2) ]\n",
    "    for i, sc in enumerate(split_corp2):\n",
    "        sent = '[SEP]'.join(sc)\n",
    "        if sent[:5] !=['[CLS]']:\n",
    "            sent = '[CLS] ' +sent if i==0 else '[CLS]' +sent\n",
    "        if sent[-5:] != '[SEP]':\n",
    "            sent += ' [SEP]'\n",
    "        sent = sent.replace('  ',' ')\n",
    "        if sent != '[CLS] [SEP]':\n",
    "            result.append( sent )\n",
    "    corpus = result[:]\n",
    " \n",
    "    tokenized_text = [tokenizer.tokenize(c) for c in corpus]\n",
    "    seg_ids = [(tt.index('[SEP]')+1)*[0] + (len(tt)-tt.index('[SEP]')-1)*[1] for tt in tokenized_text]\n",
    "    indexed_tokens = [tokenizer.convert_tokens_to_ids(tt) for tt in tokenized_text]\n",
    "    \n",
    "    lens = [len(tt) for tt in tokenized_text]\n",
    "    if max(lens)>500:\n",
    "        print(corpus)\n",
    "    \n",
    "    return tokenized_text, indexed_tokens, seg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(indx_list, m):\n",
    "    ##indx_list: [[]]\n",
    "    padded_indx = [indx+[0]*(m-len(indx)) if len(indx)<=m else indx[:m-1] + indx[-1:] for indx in indx_list]\n",
    "    \n",
    "    return padded_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "m=128\n",
    "all_tokenized_text, all_indexed_tokens, all_seg_ids = [],[],[]\n",
    "grouped_tokenized_text, grouped_indexed_tokens, grouped_seg_ids = [],[],[]\n",
    "for i, doc in enumerate(all_docs[5:10]):\n",
    "    post_tokenized_text, post_indexed_tokens, post_seg_ids = [], [], []\n",
    "    for post in doc[:-2]: \n",
    "        tokenized_text, indexed_tokens, seg_ids = pre_process(post)\n",
    "        all_tokenized_text += tokenized_text\n",
    "        all_indexed_tokens += indexed_tokens\n",
    "        all_seg_ids += seg_ids\n",
    "        post_tokenized_text.append(tokenized_text)\n",
    "        post_indexed_tokens.append(padding(indexed_tokens, m))\n",
    "        post_seg_ids.append(padding(seg_ids, m) )\n",
    "    grouped_tokenized_text.append(post_tokenized_text + doc[-2:])\n",
    "    grouped_indexed_tokens.append(post_indexed_tokens + doc[-2:])\n",
    "    grouped_seg_ids.append(post_seg_ids + doc[-2:]) \n",
    "\n",
    "#print(grouped_indexed_tokens[4][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 168 168\n",
      "5 5 5\n"
     ]
    }
   ],
   "source": [
    "print(len(all_tokenized_text), len(all_indexed_tokens), len(all_seg_ids))\n",
    "print(len(grouped_tokenized_text), len(grouped_indexed_tokens), len(grouped_seg_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding the tokenized indexed tokens and seg_ids FOR training\n",
    "all_tokenized_text2, all_indexed_tokens2, all_seg_ids2 = [],[],[]\n",
    "for indx, text, seg in zip(all_indexed_tokens, all_tokenized_text, all_seg_ids):\n",
    "    if len(indx)>m:\n",
    "        all_tokenized_text2.append(text)\n",
    "        all_indexed_tokens2.append(indx[:m-1] + indx[-1:])\n",
    "        all_seg_ids2.append(seg[:m-1] + seg[-1:])\n",
    "    else:\n",
    "        all_tokenized_text2.append(text)\n",
    "        all_indexed_tokens2.append(indx+[0]*(m-len(indx)))\n",
    "        all_seg_ids2.append(seg+[0]*(m-len(indx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor(all_indexed_tokens2)\n",
    "segments_tensors = torch.tensor(all_seg_ids2)\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107825 349533\n"
     ]
    }
   ],
   "source": [
    "all_tokenized_post, all_indexed_post, all_seg_post = [],[],[]\n",
    "all_title_url = []\n",
    "k=0\n",
    "for i, doc in enumerate(all_docs):\n",
    "    num_post = len(doc[:-2])\n",
    "    title_url = doc[-2:]\n",
    "    all_tokenized_post.append( all_tokenized_text2[k:(num_post+k)] + title_url ) \n",
    "    all_indexed_post.append( all_indexed_tokens2[k:(num_post+k)] + title_url ) # ..token2 means it's padded\n",
    "    all_seg_post.append( all_seg_ids2[k:(num_post+k)] + title_url )\n",
    "    k += num_post\n",
    "\n",
    "print(k, len(all_indexed_tokens2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "128 128\n",
      "[101, 18929, 6110, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "search = 'rotor replacement'\n",
    "tokenized_search, indexed_search, seg_search_ids = pre_process(search)\n",
    "print(len(indexed_search), len(seg_search_ids))\n",
    "\n",
    "# padding the input to make it same dimension\n",
    "indexed_search = padding(indexed_search, m)\n",
    "seg_search_ids = padding(seg_search_ids, m)\n",
    "print(len(indexed_search[0]), len(seg_search_ids[0]))\n",
    "\n",
    "tokens_search_tensor = torch.tensor(indexed_search)\n",
    "seg_search_tensor = torch.tensor(seg_search_ids)\n",
    "\n",
    "print(indexed_search[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search_vec_sum = []\\nfor token in token_embeddings:\\n    sum_vec = torch.sum(token[-4:], dim=0)\\n    search_vec_sum.append(sum_vec)\\n\\nsearch_vec = torch.mean(search_vec_sum, dim=0)\\nprint(len(search_vec))\\n#print(len(search_vec_sum), len(search_vec_sum[0]))'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_search_tensor, seg_search_tensor)\n",
    "    \n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "'''search_vec_sum = []\n",
    "for token in token_embeddings:\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    search_vec_sum.append(sum_vec)\n",
    "\n",
    "search_vec = torch.mean(search_vec_sum, dim=0)\n",
    "print(len(search_vec))\n",
    "#print(len(search_vec_sum), len(search_vec_sum[0]))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "search_token_vecs = encoded_layers[11][0]\n",
    "search_emb = torch.mean(search_token_vecs, dim=0)\n",
    "print(np.shape(np.array(search_emb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 13\n",
      "3 3\n",
      "3 3\n",
      "2 2\n",
      "1 1\n",
      "3 3\n",
      "3 3\n",
      "2 2\n",
      "2 2\n",
      "4 4\n",
      "4 4\n",
      "9 9\n",
      "3 3\n",
      "4 4\n",
      "2 2\n",
      "1 1\n",
      "3 3\n",
      "3 3\n",
      "5 5\n",
      "2 2\n",
      "1 1\n",
      "2 2\n",
      "2 2\n",
      "5 5\n",
      "4 4\n",
      "2 2\n",
      "4 4\n",
      "1 1\n",
      "2 2\n",
      "6 6\n",
      "2 2\n",
      "1 1\n",
      "2 2\n",
      "4 4\n",
      "4 4\n",
      "3 3\n",
      "2 2\n",
      "2 2\n",
      "4 4\n",
      "3 3\n",
      "1 1\n",
      "1 1\n",
      "4 4\n",
      "5 5\n",
      "2 2\n",
      "2 2\n",
      "5 5\n",
      "5 5\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "4 4\n",
      "1 1\n",
      "5 5\n"
     ]
    }
   ],
   "source": [
    "doc_emb = []\n",
    "for indexed_tokens, seg_ids in zip(grouped_indexed_tokens, grouped_seg_ids):\n",
    "    temp = 0\n",
    "    for indx, ids in zip(indexed_tokens[:-2], seg_ids[:-2]):\n",
    "        #print(len(ids),len(indx))\n",
    "        index_tensor = torch.tensor(indx)\n",
    "        seg_tensor = torch.tensor(ids)\n",
    "        with torch.no_grad():\n",
    "            encoded_layers, _ = model(index_tensor, seg_tensor)\n",
    "        token_vecs = encoded_layers[11][0]\n",
    "        emb = torch.mean(token_vecs, dim=0)\n",
    "        temp += np.array(emb)\n",
    "    \n",
    "    post_emb = temp/len(indexed_tokens[:-2])\n",
    "    doc_emb.append([seg_ids[-2], seg_ids[-1], post_emb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
